{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "vFEr6spy8mYC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ackp5rrQsMC2"
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip show openai"
   ],
   "metadata": {
    "id": "ARoC5k4_skYI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai==1.29"
   ],
   "metadata": {
    "id": "Lle8vSUZtJIu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "/content# export OPENAI_API_KEY=YourKey\n",
    "\n",
    "/content# echo $OPENAI_API_KEY\n",
    "YourKey\n"
   ],
   "metadata": {
    "id": "axM_3NMn_4aX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "gvFHHB08_s6l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "import argparse"
   ],
   "metadata": {
    "id": "s8Hauvk9-Jsh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install --upgrade --force-reinstall openai==1.29 httpx==0.27.0\n",
    "import importlib\n",
    "import openai\n",
    "import httpx\n",
    "\n",
    "# Ensure the newly installed openai and httpx modules are reloaded\n",
    "importlib.reload(openai)\n",
    "importlib.reload(httpx)\n",
    "\n",
    "# Create a default httpx client explicitly, without passing proxies\n",
    "# This bypasses OpenAI's internal httpx client creation logic that might be causing the TypeError\n",
    "default_http_client = httpx.Client()\n",
    "\n",
    "client = openai.OpenAI(api_key=\"\", http_client=default_http_client)"
   ],
   "metadata": {
    "id": "pOO3Kn-w-Vq7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_prompt(text):\n",
    "  \"\"\"\n",
    "  Create a prompt for the language model.\n",
    "  Args:\n",
    "    text: The text to classify.\n",
    "  Returns:\n",
    "    A string prompt.\n",
    "  \"\"\"\n",
    "  instructions = 'Classifique o sentimento do texto como \"positivo or negativo?'\n",
    "  formatting = '\"Positvivo\" or \"Negativo\"'\n",
    "  return f'{instructions}\\n{text}\\nAnswer ({formatting}:)'"
   ],
   "metadata": {
    "id": "yI9dNQZeXqgc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def call_llm(prompt):\n",
    "  \"\"\"\n",
    "  Call the language model.\n",
    "  Args:\n",
    "    prompt: The prompt to send to the language model.\n",
    "    Returns:\n",
    "    The response from the language model.\n",
    "  \"\"\"\n",
    "  messages = [\n",
    "\n",
    "       {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "  response = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "  return response.choices[0].message.content"
   ],
   "metadata": {
    "id": "y-XSc784XtHx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from ast import parse\n",
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  # Make 'text' argument optional for interactive execution, with a default value\n",
    "  parser.add_argument(\"text\", type=str, nargs='?', default=None, help ='A text to classify')\n",
    "\n",
    "  # In Colab/Jupyter, pass an empty list to parse_args()\n",
    "  # to prevent it from trying to parse kernel-specific arguments like '-f'.\n",
    "  args = parser.parse_args(args=[])\n",
    "  prompt =  create_prompt(args.text)\n",
    "  answer = call_llm(prompt)\n",
    "  print(answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "sOtqwj79-1kS",
    "outputId": "2293b1a0-0723-40b0-a367-4262d1b26e50"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-2481911334.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m   \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m   \u001B[0mprompt\u001B[0m \u001B[0;34m=\u001B[0m  \u001B[0mcreate_prompt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m   \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_llm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-111429306.py\u001B[0m in \u001B[0;36mcall_llm\u001B[0;34m(prompt)\u001B[0m\n\u001B[1;32m     11\u001B[0m        {\"role\": \"user\", \"content\": prompt}]\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m   \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompletions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessages\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmessages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"gpt-3.5-turbo\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchoices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m                         \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    276\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 277\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    278\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m  \u001B[0;31m# type: ignore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    588\u001B[0m         \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0mhttpx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTimeout\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0mNotGiven\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNOT_GIVEN\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    589\u001B[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[0;32m--> 590\u001B[0;31m         return self._post(\n\u001B[0m\u001B[1;32m    591\u001B[0m             \u001B[0;34m\"/chat/completions\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    592\u001B[0m             body=maybe_transform(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mpost\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1238\u001B[0m             \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"post\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mto_httpx_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1239\u001B[0m         )\n\u001B[0;32m-> 1240\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mResponseT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream_cls\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream_cls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1242\u001B[0m     def patch(\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    919\u001B[0m         \u001B[0mstream_cls\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0m_StreamT\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    920\u001B[0m     ) -> ResponseT | _StreamT:\n\u001B[0;32m--> 921\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m    922\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    923\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1003\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mretries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1004\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1005\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1006\u001B[0m                     \u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1051\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1053\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1054\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1003\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mretries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1004\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1005\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1006\u001B[0m                     \u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1051\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1053\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1054\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1018\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1019\u001B[0m             \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Re-raising status error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1020\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_status_error_from_response\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1021\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1022\u001B[0m         return self._process_response(\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7b2c200",
    "outputId": "ebcf171e-fe5d-4521-d6a1-662d461c3c5b"
   },
   "source": [
    "# try:\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"Você é um assistente prestativo.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Qual é a capital da França?\"}\n",
    "#         ]\n",
    "#     )\n",
    "#     print(response.choices[0].message.content)\n",
    "# except Exception as e:\n",
    "#     print(f\"Ocorreu um erro ao fazer a chamada da API: {e}\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ocorreu um erro ao fazer a chamada da API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n"
   ],
   "metadata": {
    "id": "iruORMewUluV"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
